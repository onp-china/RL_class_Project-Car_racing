"""
å¤šè½¦æ¯”èµ›æ¸¸æˆè¿è¡Œè„šæœ¬
æ”¯æŒ Agent vs Agent å’Œ Human vs Agent ä¸¤ç§æ¨¡å¼
"""
import os
import sys
import argparse
import collections
import numpy as np
import torch
import pygame
from typing import Optional, Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from multi_car_racing import MultiCarRacing


class FrameStacker:
    """
    æ‰‹åŠ¨å®ç° FrameStackï¼Œç”¨äºéœ€è¦4å¸§å†å²çš„æ¨¡å‹ï¼ˆDQN, SARSA, Double-DQNï¼‰
    """
    def __init__(self, n_stack=4, shape=(96, 96, 3)):
        self.n_stack = n_stack
        self.buffer = collections.deque(maxlen=n_stack)
        # åˆå§‹åŒ–æ—¶å¡«æ»¡å…¨é»‘å¸§
        for _ in range(n_stack):
            self.buffer.append(np.zeros(shape, dtype=np.uint8))
    
    def update(self, obs):
        """æ¥æ”¶æ–°çš„ä¸€å¸§ç”»é¢"""
        self.buffer.append(obs.copy() if hasattr(obs, 'copy') else obs)
    
    def get_stack(self):
        """
        è¿”å›æ‹¼æ¥å¥½çš„4å¸§ç”»é¢
        å½¢çŠ¶: (96, 96, 3) -> (96, 96, 12)
        """
        return np.concatenate(list(self.buffer), axis=-1)
    
    def reset(self):
        """é‡ç½®å¸§å†å²"""
        self.buffer.clear()
        for _ in range(self.n_stack):
            self.buffer.append(np.zeros((96, 96, 3), dtype=np.uint8))


def discrete_to_continuous(action):
    """
    å°†ç¦»æ•£åŠ¨ä½œè½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ [steer, gas, brake]
    0: nothing -> [0, 0, 0]
    1: left -> [-1, 0, 0]
    2: right -> [+1, 0, 0]
    3: gas -> [0, 1, 0]
    4: brake -> [0, 0, 0.8]
    """
    if action == 0:
        return np.array([0.0, 0.0, 0.0])
    elif action == 1:
        return np.array([-1.0, 0.0, 0.0])
    elif action == 2:
        return np.array([+1.0, 0.0, 0.0])
    elif action == 3:
        return np.array([0.0, 1.0, 0.0])
    elif action == 4:
        return np.array([0.0, 0.0, 0.8])
    else:
        return np.array([0.0, 0.0, 0.0])


def load_agent(algorithm: str, model_path: str, action_dim: int, lr: float, gamma: float, device: torch.device):
    """
    åŠ¨æ€åŠ è½½ä¸åŒç®—æ³•çš„ Agent
    
    Args:
        algorithm: ç®—æ³•åç§° ("DQN", "SARSA", "Double-DQN", "A2C", "REINFORCE")
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        action_dim: åŠ¨ä½œç»´åº¦
        lr: å­¦ä¹ ç‡ï¼ˆç”¨äºåˆå§‹åŒ–ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰
        gamma: æŠ˜æ‰£å› å­ï¼ˆç”¨äºåˆå§‹åŒ–ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰
        device: è®¾å¤‡
    
    Returns:
        Agent å¯¹è±¡
    """
    algorithm_map = {
        "DQN": ("DQN", "DQNAgent"),
        "SARSA": ("N-Step_SARSA", "NStepSarsaAgent"),
        "Double-DQN": ("Double_DQN", "DoubleDQNAgent"),
        "A2C": ("A2C", "A2CAgent"),
        "REINFORCE": ("REINFORCE", "REINFORCEAgent"),
        "PPO": ("PPO/PPO", "PPOAgent"),
    }
    
    if algorithm not in algorithm_map:
        raise ValueError(f"Unknown algorithm: {algorithm}. Available: {list(algorithm_map.keys())}")
    
    folder, class_name = algorithm_map[algorithm]
    
    # æ„å»ºè·¯å¾„ï¼šä¼˜å…ˆå°è¯• Code/ ç›®å½•
    # å‡è®¾è„šæœ¬åœ¨ Code/../multi_car_game.py (å³æ ¹ç›®å½•)
    # é‚£ä¹ˆ Code ç›®å½•åœ¨ ./Code
    
    root_dir = os.path.dirname(os.path.abspath(__file__))
    module_path = os.path.join(root_dir, "Code", folder, "agent")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
    if not os.path.exists(module_path + ".py"):
         # å°è¯•æ—§çš„æ•°å­—å‰ç¼€ç»“æ„ (fallback)
         old_algorithm_map = {
            "DQN": "1. DQN",
            "SARSA": "2. SARSA",
            "Double-DQN": "3. Double-DQN",
            "A2C": "4. A2C",
            "REINFORCE": "5. REINFORCE",
         }
         if algorithm in old_algorithm_map:
             folder = old_algorithm_map[algorithm]
             # å°è¯•åœ¨æ ¹ç›®å½•æˆ– Code ç›®å½•ä¸‹æŸ¥æ‰¾
             paths_to_try = [
                 os.path.join(root_dir, folder, "agent"),
                 os.path.join(root_dir, "Code", folder, "agent")
             ]
             for p in paths_to_try:
                 if os.path.exists(p + ".py"):
                     module_path = p
                     break

    # åŠ¨æ€å¯¼å…¥
    import importlib.util
    spec = importlib.util.spec_from_file_location("agent_module", f"{module_path}.py")
    if spec is None:
         # å°è¯•ç›´æ¥ä»å½“å‰ç›®å½•æŸ¥æ‰¾ï¼ˆå¦‚æœç»“æ„ä¸åŒï¼‰
         print(f"Warning: Could not find module at {module_path}.py, trying relative import...")
         module_path = os.path.join(os.path.dirname(__file__), folder.split(". ")[1], "agent")
         spec = importlib.util.spec_from_file_location("agent_module", f"{module_path}.py")

    agent_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(agent_module)
    
    AgentClass = getattr(agent_module, class_name)
    
    print(f"Initializing {algorithm} agent on device {device}...")
    
    # åˆ›å»º Agent å®ä¾‹
    if algorithm in ["DQN", "SARSA", "Double-DQN"]:
        # è¿™äº›ç®—æ³•éœ€è¦ buffer_size, batch_size ç­‰å‚æ•°
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾è¿™äº› Agent çš„ __init__ æ¥å—ä½ç½®å‚æ•°
        # æœ€å¥½æ£€æŸ¥ä¸€ä¸‹ Code/DQN/agent.py ç­‰
        # æš‚æ—¶ä¿æŒåŸæ ·ï¼Œå¦‚æœä¸æŠ¥é”™
        agent = AgentClass(action_dim, lr, 10000, 64, gamma, 1000, device)
    else:
        # A2C, REINFORCE, PPO
        # ä½¿ç”¨å…³é”®å­—å‚æ•°ä»¥é¿å…ä½ç½®é”™è¯¯
        agent = AgentClass(action_dim, lr=lr, gamma=gamma, device=device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    if model_path and os.path.exists(model_path):
        if algorithm in ["DQN", "SARSA", "Double-DQN"]:
            agent.q_net.load_state_dict(torch.load(model_path, map_location=device))
            agent.q_net.eval()
        elif algorithm == "A2C":
            agent.network.load_state_dict(torch.load(model_path, map_location=device))
            agent.network.eval()
        elif algorithm == "REINFORCE":
            agent.network.load_state_dict(torch.load(model_path, map_location=device))
            agent.network.eval()
        print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    else:
        print(f"âš  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    return agent, algorithm


def get_action_from_agent(agent, algorithm: str, obs: np.ndarray, use_framestack: bool, frame_stacker: Optional[FrameStacker] = None, epsilon: float = 0.0):
    """
    ä» Agent è·å–åŠ¨ä½œ
    """
    if use_framestack:
        # æ›´æ–°å¸§å†å²
        frame_stacker.update(obs)
        # è·å–å †å çš„è§‚å¯Ÿ
        stacked_obs = frame_stacker.get_stack()
        
        if algorithm in ["DQN", "SARSA", "Double-DQN"]:
            # ç¦»æ•£åŠ¨ä½œæ¨¡å‹
            action = agent.get_action(stacked_obs, epsilon)
            return action
        else:
            # è¿ç»­åŠ¨ä½œæ¨¡å‹ï¼ˆä¸åº”è¯¥ä½¿ç”¨ FrameStackï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ï¼‰
            return agent.get_action(stacked_obs)
    else:
        # å•å¸§æ¨¡å‹
        if algorithm in ["A2C", "REINFORCE"]:
            return agent.get_action(obs)
        else:
            # ç¦»æ•£åŠ¨ä½œæ¨¡å‹ä½†ä¸ç”¨ FrameStackï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
            return agent.get_action(obs, epsilon)


def get_human_action(keys: pygame.key.ScancodeWrapper) -> np.ndarray:
    """
    ä»é”®ç›˜è¾“å…¥è·å–äººç±»ç©å®¶çš„åŠ¨ä½œ
    """
    action = np.array([0.0, 0.0, 0.0], dtype=np.float32)
    
    # è½¬å‘
    if keys[pygame.K_LEFT] or keys[pygame.K_a]:
        action[0] = -1.0
    if keys[pygame.K_RIGHT] or keys[pygame.K_d]:
        action[0] = 1.0
    
    # æ²¹é—¨
    if keys[pygame.K_UP] or keys[pygame.K_w]:
        action[1] = 1.0
    
    # åˆ¹è½¦
    if keys[pygame.K_DOWN] or keys[pygame.K_s]:
        action[2] = 1.0
    
    return action


def find_latest_model(model_dir: str, pattern: str = "*final*.pth") -> Optional[str]:
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    if not os.path.exists(model_dir):
        return None
    
    import glob
    model_files = glob.glob(os.path.join(model_dir, pattern))
    if not model_files:
        # å°è¯•æŸ¥æ‰¾ checkpoint
        model_files = glob.glob(os.path.join(model_dir, "*checkpoint*.pth"))
    
    if not model_files:
        return None
    
    # è¿”å›æœ€æ–°çš„æ–‡ä»¶ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´ï¼‰
    return max(model_files, key=os.path.getmtime)


def run_race(
    mode: str = "agent_vs_agent",
    car0_config: Optional[Dict] = None,
    car1_config: Optional[Dict] = None,
    num_episodes: int = 5,
    max_steps: int = 1000,
    render: bool = True,
):
    """
    è¿è¡Œå¤šè½¦æ¯”èµ›
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # if device.type == "cpu" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    #     device = torch.device("mps")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"Device type: {device.type}, str: {str(device)}")
    print(f"Torch version: {torch.__version__}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = MultiCarRacing(
        num_agents=2,
        continuous=True,  # ç»Ÿä¸€ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´
        render_mode="human" if render else None
    )
    
    # åŠ è½½æ¨¡å‹
    agents = [None, None]
    algorithms = [None, None]
    use_framestack = [False, False]
    frame_stackers = [None, None]
    
    def setup_agent(idx, config):
        if not config: return
        
        algo = config.get("algorithm", "A2C")
        model_path = config.get("model_path")
        
        if not model_path:
            # è‡ªåŠ¨æŸ¥æ‰¾
            algo_folder_map = {
                "A2C": "A2C",
                "REINFORCE": "REINFORCE",
                "DQN": "DQN",
                "SARSA": "N-Step_SARSA",
                "Double-DQN": "Double_DQN",
                "PPO": "PPO/PPO"
            }
            if algo in algo_folder_map:
                model_dir = os.path.join(
                    os.path.dirname(__file__),
                    "Code",
                    algo_folder_map[algo],
                    "models"
                )
                if not os.path.exists(model_dir):
                     # Try without Code
                     model_dir = os.path.join(
                        os.path.dirname(__file__),
                        algo_folder_map[algo],
                        "models"
                    )
                model_path = find_latest_model(model_dir)
        
        # ç¡®å®š Action Dim
        if algo in ["DQN", "SARSA", "Double-DQN"]:
            action_dim = 5
        else:
            action_dim = 3
            
        agents[idx], algorithms[idx] = load_agent(
            algo,
            model_path,
            action_dim,
            config.get("lr", 0.0003),
            config.get("gamma", 0.99),
            device
        )
        use_framestack[idx] = (algo in ["DQN", "SARSA", "Double-DQN"])
        if use_framestack[idx]:
            frame_stackers[idx] = FrameStacker()
        print(f"è½¦è¾†{idx}: {algo} æ¨¡å‹")

    # è½¦è¾†0é…ç½®
    if mode == "human_vs_agent":
        print("è½¦è¾†0: äººç±»ç©å®¶ï¼ˆä½¿ç”¨ WASD æˆ–æ–¹å‘é”®æ§åˆ¶ï¼‰")
        agents[0] = None
    else:
        setup_agent(0, car0_config)
    
    # è½¦è¾†1é…ç½®
    setup_agent(1, car1_config)
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    total_rewards = [[], []]
    
    print(f"\nå¼€å§‹æ¯”èµ›ï¼Œå…± {num_episodes} å›åˆ...")
    print("=" * 50)
    
    for episode in range(num_episodes):
        observations, infos = env.reset()
        
        # é‡ç½®å¸§å†å²
        for i in range(2):
            if frame_stackers[i] is not None:
                frame_stackers[i].reset()
                # åˆå§‹åŒ–æ—¶å¡«å…¥ç¬¬ä¸€å¸§
                frame_stackers[i].update(observations[i])
        
        episode_rewards = [0.0, 0.0]
        
        running = True
        step_count = 0
        
        while running and step_count < max_steps:
            # å¤„ç† pygame äº‹ä»¶ï¼ˆç”¨äºäººç±»è¾“å…¥å’Œçª—å£å…³é—­ï¼‰
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    break
            
            if not running:
                break
            
            # è·å–åŠ¨ä½œ
            actions = []
            for i in range(2):
                if agents[i] is None:
                    # äººç±»ç©å®¶
                    keys = pygame.key.get_pressed()
                    action = get_human_action(keys)
                else:
                    # AI Agent
                    action = get_action_from_agent(
                        agents[i],
                        algorithms[i],
                        observations[i],
                        use_framestack[i],
                        frame_stackers[i],
                        epsilon=0.0  # æµ‹è¯•æ—¶ä½¿ç”¨è´ªå©ªç­–ç•¥
                    )
                    
                    # å¦‚æœæ˜¯ç¦»æ•£åŠ¨ä½œï¼ˆæ•´æ•°ï¼‰ï¼Œè½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ
                    if isinstance(action, (int, np.integer)):
                        action = discrete_to_continuous(action)
                        
                actions.append(action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            observations, rewards, terminations, truncations, infos = env.step(actions)
            
            # æ›´æ–°å¸§å†å²
            for i in range(2):
                if frame_stackers[i] is not None:
                    frame_stackers[i].update(observations[i])
            
            # ç´¯è®¡å¥–åŠ±
            for i in range(2):
                episode_rewards[i] += rewards[i]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è½¦å®Œæˆæ¯”èµ›
            lap_finished = any(info.get("lap_finished", False) for info in infos)
            if lap_finished:
                print("\nğŸ æ¯”èµ›ç»“æŸï¼æœ‰è½¦è¾†å®Œæˆæ¯”èµ›ã€‚")
                break
                
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è½¦éƒ½ç»ˆæ­¢ï¼ˆä¾‹å¦‚å‡ºç•Œæˆ–å®Œæˆï¼‰
            if all(terminations) or all(truncations):
                print("\nğŸ›‘ æ¯”èµ›ç»“æŸï¼æ‰€æœ‰è½¦è¾†ç»ˆæ­¢ã€‚")
                break
            
            step_count += 1
        
        # è®°å½•æœ¬è½®å¥–åŠ±
        print(f"å›åˆ {episode+1}/{num_episodes}:")
        for i in range(2):
            total_rewards[i].append(episode_rewards[i])
            player_name = "äººç±»" if agents[i] is None else f"Agent({algorithms[i]})"
            lap_status = "å®Œæˆ" if infos[i].get("lap_finished", False) else "æœªå®Œæˆ"
            print(f"  {player_name}: {episode_rewards[i]:.2f} ({lap_status})")
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 50)
    print("=== æ¯”èµ›ç»“æœç»Ÿè®¡ ===")
    print("=" * 50)
    
    for i in range(2):
        player_name = "äººç±»" if agents[i] is None else f"Agent({algorithms[i]})"
        rewards_list = total_rewards[i]
        if rewards_list:
            avg_reward = np.mean(rewards_list)
            std_reward = np.std(rewards_list)
            max_reward = np.max(rewards_list)
            min_reward = np.min(rewards_list)
            
            print(f"\n{player_name}:")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f} Â± {std_reward:.2f}")
            print(f"  æœ€é«˜å¥–åŠ±: {max_reward:.2f}")
            print(f"  æœ€ä½å¥–åŠ±: {min_reward:.2f}")
    
    env.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¤šè½¦æ¯”èµ›æ¸¸æˆ")
    parser.add_argument("--mode", type=str, default="agent_vs_agent", 
                       choices=["agent_vs_agent", "human_vs_agent"],
                       help="æ¯”èµ›æ¨¡å¼")
    parser.add_argument("--car0_algorithm", type=str, default="A2C",
                       choices=["DQN", "SARSA", "Double-DQN", "A2C", "REINFORCE"],
                       help="è½¦è¾†0çš„ç®—æ³•")
    parser.add_argument("--car0_model", type=str, default=None,
                       help="è½¦è¾†0çš„æ¨¡å‹è·¯å¾„ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")
    parser.add_argument("--car1_algorithm", type=str, default="REINFORCE",
                       choices=["DQN", "SARSA", "Double-DQN", "A2C", "REINFORCE"],
                       help="è½¦è¾†1çš„ç®—æ³•")
    parser.add_argument("--car1_model", type=str, default=None,
                       help="è½¦è¾†1çš„æ¨¡å‹è·¯å¾„ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")
    parser.add_argument("--episodes", type=int, default=5,
                       help="æ¯”èµ›å›åˆæ•°")
    parser.add_argument("--max_steps", type=int, default=1000,
                       help="æ¯å›åˆæœ€å¤§æ­¥æ•°")
    parser.add_argument("--no_render", action="store_true",
                       help="ä¸æ¸²æŸ“ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    car0_config = None
    car1_config = None
    
    if args.mode == "human_vs_agent":
        # äººç±» vs Agent
        car1_config = {
            "algorithm": args.car1_algorithm,
            "model_path": args.car1_model,
            "lr": 0.0003,
            "gamma": 0.99
        }
    else:
        # Agent vs Agent
        car0_config = {
            "algorithm": args.car0_algorithm,
            "model_path": args.car0_model,
            "lr": 0.0003,
            "gamma": 0.99
        }
        car1_config = {
            "algorithm": args.car1_algorithm,
            "model_path": args.car1_model,
            "lr": 0.0003,
            "gamma": 0.99
        }
    
    run_race(
        mode=args.mode,
        car0_config=car0_config,
        car1_config=car1_config,
        num_episodes=args.episodes,
        max_steps=args.max_steps,
        render=not args.no_render
    )
