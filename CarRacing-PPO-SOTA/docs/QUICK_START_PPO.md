
# PPO Training for CarRacing - Quick Start

## ğŸ¯ PPO ç®€ä»‹

PPO (Proximal Policy Optimization) æ˜¯ç›®å‰æœ€æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç‰¹åˆ«é€‚åˆ CarRacingï¼š

âœ… **è¿ç»­æ§åˆ¶** - ç›´æ¥è¾“å‡ºå¹³æ»‘çš„æ–¹å‘ç›˜/æ²¹é—¨/åˆ¹è½¦æ§åˆ¶  
âœ… **ç¨³å®šè®­ç»ƒ** - ä¸å®¹æ˜“å´©æºƒï¼Œé€‚åˆæ–°æ‰‹  
âœ… **é«˜æ€§èƒ½** - åœ¨èµ›è½¦æ¸¸æˆä¸­è¡¨ç°ä¼˜å¼‚  
âœ… **å·¥ä¸šæ ‡å‡†** - OpenAIã€DeepMind ç­‰éƒ½åœ¨ä½¿ç”¨  

---

## ğŸ“¦ å®‰è£…ä¾èµ–

```powershell
# ç¡®ä¿å·²å®‰è£…è¿™äº›åº“ï¼ˆåŒ…å«è¿›åº¦æ¡æ”¯æŒï¼‰
py -m pip install gymnasium[box2d] torch opencv-python numpy tqdm

# æˆ–è€…ä½¿ç”¨ requirements.txtï¼ˆæ¨èï¼‰
py -m pip install -r requirements.txt
```

---

## ğŸš€ å¼€å§‹è®­ç»ƒ

### **æ–¹å¼ 1: å•ç¯å¢ƒè®­ç»ƒï¼ˆæ¨èå…¥é—¨ï¼‰**

```powershell
# åŸºç¡€è®­ç»ƒ
py train_ppo.py

# è‡ªå®šä¹‰å‚æ•°
py train_ppo.py --max_episodes 500 --frame_skip 2
```

**ä¼˜ç‚¹**: ç®€å•ç¨³å®šï¼Œé€‚åˆæµ‹è¯•å’Œè°ƒè¯•  
**é€Ÿåº¦**: ä¸­ç­‰  
**å†…å­˜**: ä½  

---

### **æ–¹å¼ 2: å‘é‡åŒ–è®­ç»ƒï¼ˆæ¨èç”Ÿäº§ï¼‰âš¡**

```powershell
# å¿«é€Ÿè®­ç»ƒï¼ˆ4 å€é€Ÿåº¦ï¼‰
py train_ppo_fast.py --num_envs 4

# æ›´å¿«ï¼ˆ8 å€é€Ÿåº¦ï¼Œéœ€è¦å¥½ CPUï¼‰
py train_ppo_fast.py --num_envs 8 --rollout_steps 512
```

**ä¼˜ç‚¹**: è®­ç»ƒé€Ÿåº¦å¿« 4-8 å€  
**é€Ÿåº¦**: éå¸¸å¿« ğŸš€  
**å†…å­˜**: ä¸­ç­‰-é«˜  

---

## âš™ï¸ é‡è¦å‚æ•°è¯´æ˜

### **åŠ é€Ÿç›¸å…³**

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èèŒƒå›´ |
|------|-------|------|---------|
| `--num_envs` | 4 | å¹¶è¡Œç¯å¢ƒæ•°é‡ | 2-8ï¼ˆæ ¹æ® CPU æ ¸å¿ƒæ•°ï¼‰ |
| `--frame_skip` | 2 | åŠ¨ä½œé‡å¤æ¬¡æ•° | 2-4ï¼ˆè¶Šå¤§è¶Šå¿«ä½†ç²¾åº¦é™ä½ï¼‰ |

### **PPO è¶…å‚æ•°**

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|-------|------|
| `--lr` | 3e-4 | å­¦ä¹ ç‡ |
| `--gamma` | 0.99 | æŠ˜æ‰£å› å­ |
| `--gae_lambda` | 0.95 | GAE lambdaï¼ˆä¼˜åŠ¿ä¼°è®¡ï¼‰ |
| `--clip_epsilon` | 0.2 | PPO è£å‰ªèŒƒå›´ |
| `--ppo_epochs` | 10 | æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•° |
| `--batch_size` | 64 | Mini-batch å¤§å° |
| `--rollout_steps` | 2048 / 512 | æ¯æ¬¡æ”¶é›†çš„æ­¥æ•° |

---

## ğŸ“Š è®­ç»ƒé…ç½®å»ºè®®

### **å¿«é€Ÿæµ‹è¯•ï¼ˆç¬”è®°æœ¬ç”µè„‘ï¼‰**
```powershell
py train_ppo.py --max_episodes 200 --frame_skip 4
```
- æ—¶é—´: ~1-2 å°æ—¶
- é€‚åˆ: éªŒè¯ä»£ç æ˜¯å¦æ­£å¸¸å·¥ä½œ

### **æ ‡å‡†è®­ç»ƒï¼ˆå°å¼æœº/å·¥ä½œç«™ï¼‰**
```powershell
py train_ppo_fast.py --num_envs 4 --max_episodes 500
```
- æ—¶é—´: ~2-3 å°æ—¶
- é€‚åˆ: è·å¾—åŸºæœ¬å¯ç”¨çš„æ¨¡å‹

### **é«˜è´¨é‡è®­ç»ƒï¼ˆæœåŠ¡å™¨/GPUï¼‰**
```powershell
py train_ppo_fast.py --num_envs 8 --max_episodes 1000 --frame_skip 2
```
- æ—¶é—´: ~4-6 å°æ—¶
- é€‚åˆ: è¿½æ±‚é«˜åˆ†å’Œç¨³å®šæ€§

---

## ğŸ® æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ `test_ppo.py`:

```python
import torch
from utils_env_continuous import make_continuous_env
from ppo_agent import PPOAgent

# åŠ è½½ç¯å¢ƒ
env = make_continuous_env(render_mode="human", frame_skip=2)

# åˆ›å»ºæ™ºèƒ½ä½“å¹¶åŠ è½½æ¨¡å‹
state_dim = (4, 96, 96)
action_dim = 3
agent = PPOAgent(state_dim, action_dim)
agent.load("saved_models/ppo_carracing_ep500.pth")

# æµ‹è¯• 5 ä¸ªå›åˆ
for episode in range(5):
    state, _ = env.reset()
    episode_reward = 0
    done = truncated = False
    
    while not (done or truncated):
        # ç¡®å®šæ€§ç­–ç•¥ï¼ˆæ— éšæœºæ¢ç´¢ï¼‰
        action, _, _ = agent.get_action(state, deterministic=True)
        state, reward, done, truncated, _ = env.step(action)
        episode_reward += reward
    
    print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}")

env.close()
```

è¿è¡Œæµ‹è¯•:
```powershell
py test_ppo.py
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§ï¼ˆå«å®æ—¶è¿›åº¦æ¡ï¼‰â­

### **æ–°åŠŸèƒ½ï¼šå®æ—¶è¿›åº¦æ¡**

è®­ç»ƒæ—¶ä¼šæ˜¾ç¤ºæ¼‚äº®çš„å®æ—¶è¿›åº¦æ¡ï¼š

```
ğŸš€ Fast PPO Training: 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 450/1000 [01:23<02:15, 4.05ep/s]
Reward: 342.5, Avg100: 298.3, TotalSteps: 125340

Episode  450: Reward= 342.54, Avg100= 298.31, Steps= 892
```

**è¿›åº¦æ¡æ˜¾ç¤ºå†…å®¹ï¼š**
- âœ… å®Œæˆç™¾åˆ†æ¯” (45%) å’Œå¯è§†åŒ–è¿›åº¦æ¡
- âœ… å·²ç”¨æ—¶é—´ [01:23] å’Œé¢„è®¡å‰©ä½™æ—¶é—´ <02:15> â°
- âœ… è®­ç»ƒé€Ÿåº¦ (4.05 episodes/ç§’)
- âœ… å®æ—¶ Reward å’Œ Avg100 æŒ‡æ ‡

> ğŸ’¡ è¯¦ç»†çš„è¿›åº¦æ¡ä½¿ç”¨æŒ‡å—è¯·æŸ¥çœ‹ `PROGRESS_BAR_GUIDE.md`

### **çœ‹æ‡‚è¾“å‡º**

```
Episode  100: Reward= 245.32, Avg100= 178.45, TotalSteps=125000
```

- **Reward**: å½“å‰å›åˆå¾—åˆ†
- **Avg100**: æœ€è¿‘ 100 å›åˆå¹³å‡åˆ†ï¼ˆ**æœ€é‡è¦çš„æŒ‡æ ‡**ï¼‰
- **TotalSteps**: ç´¯è®¡è®­ç»ƒæ­¥æ•°

### **è®­ç»ƒè¿›åº¦å‚è€ƒ**

| Avg100 åˆ†æ•° | æ°´å¹³ | è¯´æ˜ |
|------------|------|------|
| < 0 | åˆæœŸ | è¿˜åœ¨å­¦ä¹ åŸºæœ¬æ§åˆ¶ |
| 0 - 200 | å…¥é—¨ | èƒ½åœ¨èµ›é“ä¸Šè·‘å‡ åœˆäº† |
| 200 - 500 | ä¸­çº§ | åŸºæœ¬æŒæ¡äº†é©¾é©¶æŠ€å·§ |
| 500 - 800 | é«˜çº§ | è¡¨ç°ä¼˜ç§€ï¼Œå¾ˆå°‘å‡ºç•Œ |
| > 800 | ä¸“å®¶ | æ¥è¿‘å®Œç¾é©¾é©¶ |

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### **Q: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ**

1. å¢åŠ  `--frame_skip` (2 â†’ 4)
2. ä½¿ç”¨å‘é‡åŒ–è®­ç»ƒ `train_ppo_fast.py`
3. å¢åŠ  `--num_envs` (4 â†’ 6 â†’ 8)
4. å‡å°‘ `--rollout_steps` (2048 â†’ 1024 â†’ 512)

### **Q: å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ**

1. å‡å°‘ `--num_envs` (8 â†’ 4 â†’ 2)
2. å‡å°‘ `--rollout_steps`
3. å‡å°‘ `--batch_size`

### **Q: è®­ç»ƒä¸æ”¶æ•› / Reward ä¸€ç›´å¾ˆä½ï¼Ÿ**

1. æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ­£ç¡®çš„ä¾èµ– (ç‰¹åˆ«æ˜¯ Box2D)
2. é™ä½å­¦ä¹ ç‡ `--lr 1e-4`
3. å¢åŠ è®­ç»ƒæ—¶é—´ (è‡³å°‘ 300-500 episodes)
4. è°ƒæ•´ `--frame_skip` (å¯èƒ½å¤ªå¤§äº†)

### **Q: GPU åˆ©ç”¨ç‡ä½ï¼Ÿ**

PPO åœ¨ CarRacing ä¸Šä¸»è¦å—é™äºç¯å¢ƒæ¨¡æ‹Ÿé€Ÿåº¦ï¼ˆBox2Dï¼‰ï¼Œè€Œä¸æ˜¯ç¥ç»ç½‘ç»œè®¡ç®—ã€‚ä½¿ç”¨å‘é‡åŒ–ç¯å¢ƒå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ CPUã€‚

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶è¯´æ˜

```
é¡¹ç›®æ ¹ç›®å½•/
â”œâ”€â”€ utils_env_continuous.py    # è¿ç»­åŠ¨ä½œç¯å¢ƒåŒ…è£…å™¨
â”œâ”€â”€ utils_model_ppo.py          # Actor-Critic ç¥ç»ç½‘ç»œ
â”œâ”€â”€ ppo_agent.py                # PPO ç®—æ³•å®ç°
â”œâ”€â”€ train_ppo.py                # å•ç¯å¢ƒè®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_ppo_fast.py           # å‘é‡åŒ–å¿«é€Ÿè®­ç»ƒ âš¡
â”œâ”€â”€ test_ppo.py                 # æµ‹è¯•è„šæœ¬ï¼ˆéœ€è¦è‡ªå·±åˆ›å»ºï¼‰
â”œâ”€â”€ saved_models/               # æ¨¡å‹ä¿å­˜ç›®å½•
â”‚   â”œâ”€â”€ ppo_carracing_ep50.pth
â”‚   â”œâ”€â”€ ppo_carracing_ep100.pth
â”‚   â””â”€â”€ ...
â””â”€â”€ QUICK_START_PPO.md         # æœ¬æ–‡æ¡£
```

---

## ğŸ“ è¿›é˜¶ä¼˜åŒ–

å¦‚æœä½ æƒ³è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒï¼š

1. **è°ƒæ•´å¥–åŠ±å‡½æ•°** - ä¿®æ”¹ç¯å¢ƒåŒ…è£…å™¨ï¼ŒåŠ å…¥è‡ªå®šä¹‰å¥–åŠ±
2. **Curriculum Learning** - å…ˆè®­ç»ƒç®€å•èµ›é“ï¼Œå†è®­ç»ƒå¤æ‚èµ›é“
3. **æ›´å¤§çš„ç½‘ç»œ** - ä¿®æ”¹ `utils_model_ppo.py` å¢åŠ ç½‘ç»œå®¹é‡
4. **æ›´å¤šè¶…å‚æ•°è°ƒä¼˜** - ä½¿ç”¨ Weights & Biases è¿›è¡Œè¶…å‚æ•°æœç´¢

---

## ğŸ’¡ ä¸ DQN çš„å¯¹æ¯”

| ç‰¹æ€§ | DQN (ç¦»æ•£) | PPO (è¿ç»­) |
|------|-----------|-----------|
| åŠ¨ä½œç©ºé—´ | 12 ä¸ªå›ºå®šåŠ¨ä½œ | è¿ç»­å¹³æ»‘æ§åˆ¶ |
| æ§åˆ¶ç²¾åº¦ | ç²—ç³™ï¼ˆé˜¶è·ƒï¼‰ | ç²¾ç»†ï¼ˆå¹³æ»‘ï¼‰ |
| è®­ç»ƒéš¾åº¦ | ç®€å• | ä¸­ç­‰ |
| æœ€ç»ˆæ€§èƒ½ | ä¸­ç­‰ | **æ›´é«˜** âœ… |
| é€‚ç”¨åœºæ™¯ | å­¦ä¹ /å¯¹æ¯” | **ç”Ÿäº§æ¨è** âœ… |

---

## âœ… æ€»ç»“

**æ¨èæµç¨‹**ï¼š

1. å…ˆç”¨ `train_ppo.py` å¿«é€Ÿæµ‹è¯• 50-100 episodesï¼Œç¡®è®¤ä»£ç èƒ½è·‘
2. å†ç”¨ `train_ppo_fast.py --num_envs 4` è®­ç»ƒ 500+ episodes
3. è§‚å¯Ÿ Avg100 åˆ†æ•°æ˜¯å¦ç¨³å®šä¸Šå‡
4. è¾¾åˆ°æ»¡æ„æ•ˆæœåï¼Œä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æµ‹è¯•

**é¢„æœŸæ—¶é—´**ï¼š
- å•ç¯å¢ƒ: 4-6 å°æ—¶ (1000 episodes)
- 4 å¹¶è¡Œ: 1-2 å°æ—¶ (1000 episodes) âš¡
- 8 å¹¶è¡Œ: 0.5-1 å°æ—¶ (1000 episodes) ğŸš€

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸï¸ğŸ’¨


