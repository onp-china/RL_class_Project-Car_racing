# PPO (Proximal Policy Optimization)

> è¿ç»­åŠ¨ä½œæ§åˆ¶ç®—æ³• - CarRacing æœ€æ¨èæ–¹æ¡ˆ â­â­â­â­â­

## ğŸ“– ç®—æ³•ç®€ä»‹

PPO æ˜¯ç›®å‰å·¥ä¸šç•Œæœ€å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œç”± OpenAI åœ¨ 2017 å¹´æå‡ºã€‚å®ƒæ˜¯ Policy Gradient ç®—æ³•çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ã€‚

### ä¼˜åŠ¿

- âœ… **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼šç›´æ¥è¾“å‡ºå¹³æ»‘çš„æ–¹å‘ç›˜/æ²¹é—¨/åˆ¹è½¦æ§åˆ¶
- âœ… **è®­ç»ƒç¨³å®š**ï¼šä¸å®¹æ˜“å´©æºƒï¼Œé€‚åˆæ–°æ‰‹
- âœ… **æ ·æœ¬æ•ˆç‡é«˜**ï¼šç›¸æ¯” DQN æ›´å¿«æ”¶æ•›
- âœ… **å·¥ä¸šæ ‡å‡†**ï¼šOpenAIã€DeepMind ç­‰éƒ½åœ¨ä½¿ç”¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å•ç¯å¢ƒè®­ç»ƒ

```bash
python -m algorithms.ppo.train --max_episodes 500
```

### å‘é‡åŒ–å¿«é€Ÿè®­ç»ƒï¼ˆæ¨èï¼‰

```bash
python -m algorithms.ppo.train_fast --num_envs 8 --frame_skip 3 --max_episodes 500
```

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `agent.py` | PPO Agent å®ç°ï¼ˆRolloutBuffer, è®­ç»ƒé€»è¾‘ï¼‰ |
| `model.py` | Actor-Critic ç¥ç»ç½‘ç»œï¼ˆCNN + ç­–ç•¥ç½‘ç»œ + ä»·å€¼ç½‘ç»œï¼‰ |
| `env_wrapper.py` | è¿ç»­åŠ¨ä½œç¯å¢ƒåŒ…è£…å™¨ï¼ˆå¸§å †å ã€Frame Skipï¼‰ |
| `train.py` | å•ç¯å¢ƒè®­ç»ƒè„šæœ¬ |
| `train_fast.py` | å‘é‡åŒ–å¿«é€Ÿè®­ç»ƒè„šæœ¬ â­ |

---

## âš™ï¸ å‚æ•°è¯´æ˜

### è®­ç»ƒå‚æ•°

```bash
python -m algorithms.ppo.train_fast \
    --num_envs 8 \              # å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆ4-8 æ¨èï¼‰
    --frame_skip 3 \            # å¸§è·³è·ƒï¼ˆ2-4 æ¨èï¼‰
    --max_episodes 500 \        # æ€»è®­ç»ƒå›åˆæ•°
    --rollout_steps 512 \       # æ¯æ¬¡é‡‡é›†æ­¥æ•°
    --batch_size 64 \           # Mini-batch å¤§å°
    --ppo_epochs 10 \           # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
    --lr 3e-4 \                 # å­¦ä¹ ç‡
    --gamma 0.99 \              # æŠ˜æ‰£å› å­
    --gae_lambda 0.95 \         # GAE lambda
    --clip_epsilon 0.2 \        # PPO è£å‰ªèŒƒå›´
    --save_freq 50 \            # ä¿å­˜é¢‘ç‡
    --eval_freq 100             # è¯„ä¼°é¢‘ç‡
```

### å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | é»˜è®¤å€¼ | è°ƒä¼˜å»ºè®® |
|------|-------|---------|
| `lr` | 3e-4 | å¦‚æœä¸æ”¶æ•›é™åˆ° 1e-4 |
| `clip_epsilon` | 0.2 | è®­ç»ƒä¸ç¨³å®šå¯æ”¹ä¸º 0.1 |
| `ppo_epochs` | 10 | æ ·æœ¬å°‘æ—¶å¯å¢åŠ åˆ° 15-20 |
| `rollout_steps` | 512 | å†…å­˜å¤Ÿå¯å¢åŠ åˆ° 1024-2048 |

---

## ğŸ“Š è®­ç»ƒæ•ˆæœ

### å…¸å‹è®­ç»ƒæ›²çº¿

```
Episode  100: Reward= 180.23, Avg100= 145.32
Episode  200: Reward= 425.67, Avg100= 298.45
Episode  300: Reward= 556.89, Avg100= 412.78
Episode  500: Reward= 678.90, Avg100= 567.89
```

### æ€§èƒ½åŸºå‡†

| è®­ç»ƒå›åˆ | Avg100 åˆ†æ•° | è®­ç»ƒæ—¶é—´ï¼ˆ8 envsï¼‰ |
|---------|------------|------------------|
| 100 | 150-250 | ~10 åˆ†é’Ÿ |
| 300 | 400-500 | ~30 åˆ†é’Ÿ |
| 500 | 550-650 | ~50 åˆ†é’Ÿ |
| 1000 | 700-800+ | ~1.5-2 å°æ—¶ |

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### 1. è®­ç»ƒæ–°æ¨¡å‹

```bash
# æ ‡å‡†é…ç½®ï¼ˆå¹³è¡¡é€Ÿåº¦ä¸æ€§èƒ½ï¼‰
python -m algorithms.ppo.train_fast --num_envs 8 --frame_skip 3 --max_episodes 500

# å¿«é€Ÿæµ‹è¯•ï¼ˆ200 episodesï¼‰
python -m algorithms.ppo.train_fast --num_envs 8 --frame_skip 4 --max_episodes 200

# é«˜è´¨é‡è®­ç»ƒ
python -m algorithms.ppo.train_fast --num_envs 6 --frame_skip 2 --max_episodes 1000
```

### 2. åŠ è½½å¹¶æµ‹è¯•æ¨¡å‹

```python
from algorithms.ppo import PPOAgent, make_continuous_env

# åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
env = make_continuous_env(render_mode="human")
agent = PPOAgent(state_dim=(4, 96, 96), action_dim=3)

# åŠ è½½æ¨¡å‹
agent.load("../../saved_models/ppo/ppo_fast_carracing_ep500.pth")

# æµ‹è¯•
state, _ = env.reset()
for _ in range(1000):
    action, _, _ = agent.get_action(state, deterministic=True)
    state, reward, done, truncated, _ = env.step(action)
    if done or truncated:
        break
```

### 3. ç»§ç»­è®­ç»ƒ

```bash
# ä»æ£€æŸ¥ç‚¹ç»§ç»­ï¼ˆéœ€è¦æ·»åŠ  --resume å‚æ•°ï¼‰
python -m algorithms.ppo.train_fast --num_envs 8 --resume saved_models/ppo/ppo_fast_carracing_ep500.pth
```

---

## ğŸ”§ è‡ªå®šä¹‰ä¿®æ”¹

### ä¿®æ”¹ç½‘ç»œç»“æ„

ç¼–è¾‘ `model.py` ä¸­çš„ `CNNBase`:

```python
class CNNBase(nn.Module):
    def __init__(self, input_channels=4):
        super().__init__()
        # ä¿®æ”¹è¿™é‡Œçš„ç½‘ç»œå±‚
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
```

### ä¿®æ”¹å¥–åŠ±å‡½æ•°

ç¼–è¾‘ `env_wrapper.py` ä¸­çš„ `step` æ–¹æ³•ï¼š

```python
def step(self, action):
    # ... æ‰§è¡ŒåŠ¨ä½œ ...
    
    # è‡ªå®šä¹‰å¥–åŠ±å¡‘é€ 
    shaped_reward = total_reward
    if is_on_grass:
        shaped_reward -= 1.0  # æƒ©ç½šè·‘åˆ°è‰åœ°
    if speed > threshold:
        shaped_reward += 0.1  # å¥–åŠ±é«˜é€Ÿ
    
    return self.frames.copy(), shaped_reward, terminated, truncated, info
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### åŠ é€ŸæŠ€å·§

1. **å¢åŠ å¹¶è¡Œç¯å¢ƒ**
   ```bash
   --num_envs 12  # å¦‚æœ CPU æ ¸å¿ƒå¤Ÿ
   ```

2. **è°ƒå¤§ Frame Skip**
   ```bash
   --frame_skip 4  # å¿« 2 å€ä½†ç²¾åº¦ç¨é™
   ```

3. **æ›´å¤§çš„ Batch Size**
   ```bash
   --batch_size 128  # å……åˆ†åˆ©ç”¨ GPU
   ```

4. **ç»„åˆä½¿ç”¨**
   ```bash
   python -m algorithms.ppo.train_fast --num_envs 12 --frame_skip 4 --batch_size 128
   # é¢„æœŸåŠ é€Ÿï¼š15-20x âš¡âš¡âš¡
   ```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒä¸æ”¶æ•›ï¼ŒReward ä¸€ç›´æ˜¯è´Ÿæ•°ï¼Ÿ

**A:** å°è¯•ï¼š
1. é™ä½å­¦ä¹ ç‡ï¼š`--lr 1e-4`
2. å¢åŠ è®­ç»ƒæ—¶é—´ï¼šè‡³å°‘ 300+ episodes
3. æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æ­£å¸¸ï¼šè¿è¡Œ `human_play.py` æµ‹è¯•

### Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**A:** 
1. ä½¿ç”¨ `train_fast.py` è€Œä¸æ˜¯ `train.py`
2. å¢åŠ  `--num_envs` å’Œ `--frame_skip`
3. ç¡®ä¿ä½¿ç”¨äº† GPUï¼šæ£€æŸ¥è¾“å‡ºä¸­çš„ "Device: cuda"

### Q: å†…å­˜ä¸è¶³ï¼Ÿ

**A:** 
1. å‡å°‘ `--num_envs`
2. å‡å°‘ `--rollout_steps`
3. å‡å°‘ `--batch_size`

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [PPO åŸè®ºæ–‡](https://arxiv.org/abs/1707.06347)
- [OpenAI Spinning Up - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- [è¯¦ç»†æ•™ç¨‹](../../docs/PPO_GUIDE.md)

---

**å¼€å§‹è®­ç»ƒä½ çš„ PPO Agentï¼** ğŸš€

```bash
python -m algorithms.ppo.train_fast --num_envs 8 --max_episodes 500
```



